\chapter{More speed via univariate feature-screening and early-stopping}
\label{chap:speeding}
\markright{{~{\rm \ref{chap:speeding}}. More speed via univariate feature-screening and early-stopping}\hfill}{}

\minitoc

\section{Introduction}
\newthought{In our PRNI 2015} conference paper \citep{dohmatob2015speeding}, we developed some heuristics for speeding up the overall optimization process: (a) Early-stopping, whereby one  halts
the optimization process when the test score (performance on left-out
data) for the internal cross-validation for model-selection stops
improving, and (b) univariate feature-screening, whereby irrelevant
(non-predictive) voxels are detected and eliminated before the
optimization problem is entered, thus reducing the size of the
problem.

\begin{figure}[!htb]
  \includegraphics[width=1\linewidth]{figures/screening.png}
  \caption{Univariate feature-screening for the
    GraphNet~\citep{hebiri2011,grosenick2013}
    problem \eqref{eq:opt_pb} on
    different datasets.
    % $(\X,\y)$.
    This figure shows spatial maps of
    $\B{X}^T_j\y$, thresholded so that only voxels $j$ with (from left to
    rightmost column)  $|\B{X}^T_j\y| \ge p_{10\%}(|\B{X}^T\y|)$, $|\B{X}^T_j\y| \ge
    p_{20\%}(|\B{X}^T\y|)$, $|\B{X}^T_j\y| \ge p_{50\%}(|\B{X}^T\y|)$, and $|\B{X}^T_j\y|
    \ge p_{100\%}(|\B{X}^T\y|)$ (full-brain) respectively, survive. The
    green contours enclose the elite voxels which are selected by the
    screening procedure at the respective threshold
    levels. \textit{(a)}: Mixed Gambles dataset
    \citep{jimura2012}.%  Remarkably, the geometry of the regions obtained
    % here for the 10th and 20th screening-percentiles match pretty well
    % the results obtained in  \citep{gramfort2013} with their TV-L1
    % penalty. \textit{(b)}: Face vs House contrast of the visual recognition
    % dataset  \citep{haxby2001}. 
    Weights maps obtained for the GraphNet
    model \eqref{eq:opt_pb} with these different
    screening-percentiles are shown in Figure
    \ref{fig:haxby}. \textit{(c)}: OASIS dataset  \citep{marcus2007open}
    with VBM. See Figure \ref{fig:oasis} for weights maps and
    age predictions obtained using these different
    screening-percentiles. % on the GraphNet model \eqref{eq:opt_pb}.
  }
  
  \label{fig:screening}
\end{figure}

Empirical results with GraphNet on real MRI (Magnetic
Resonance Imaging)
datasets indicated that these heuristics are a win-win strategy, as
they add speed without sacrificing the quality of the predictions
/ classifications.

%% We will now detail the heuristic techiques for speeding up
%% numerical optimization of GraphNet  \citep{hebiri2011,grosenick2013}
%% model for brain data.


% \paragraph{A note on implementation of the solver.}
One notes that in the case of GraphNet, the penalty term % $J(\w)$
of problem
\eqref{eq:opt_pb}, the $\|\nabla \w\|^2_2$ sub-term is smooth (i.e differentiable) with
\textit{Lipschitz} gradient, whilst the $\ell_{1}$ term though
nonsmooth, is proximable
% \footnote{That is, there is a
% closed-form analytic expression for its proximal operator.}
by means
of the \textit{soft-thesholding} operator  \citep{daubechies2004}.  Thus
problem \eqref{eq:opt_pb} is amenable to the FISTA (Fast Iterative
Shrinkage-Thresholding Algorithm)  \citep{beck09fista}, with a provable
$\mathcal{O}(1/\sqrt{\epsilon})$ convergence rate. Our implementation
of FISTA uses technical recommendations
(line-searching, parametrization, etc.) which were provided in
 \citep{dohmatob2014benchmarking}, in the context of TV-L1
 ~\citep{baldassarre2012,gramfort2013}. The model parameters $\alpha$ and
$\rho$ in \eqref{eq:opt_pb} are set by \textit{internal}
cross-validation.

\section{Methods}
%% \paragraph*{(b) Warm-start}
%% The $(\alpha, \rho)$ parameter grid is walked $\rho$-first. For each $\rho$ value encountered, the alphas are walked from largest to smallest value. The largest value is contructed to be the largest value of $\alpha$ for which the optimal solution to problem \eqref{eq:opt_pb} is necessarily zero.

%% We now provide details on the speedup heuristics for speeding up
%% the overall implementation, including model-selection part of it.


\subsection{Univariate feature-screening}
%% \textbf{XXX: In a paragraph briefly give a comprehensive overview of
%%   screening algorithms and heuristics from El Ghaoui, upto the more
%%   recent Liu et al!!!}
In machine-learning, feature-screening aims at detecting and
eliminating irrelevant (non-predictive)
features thus reducing the size of the underlying
optimization problem (here problem \eqref{eq:opt_pb}). The general idea
is to compute for each value of the regularization parameter, a
\textit{relevance measure} for each feature, which is then compared with a
threshold (produced by the screening procedure itself). Features which fall short
of this threshold are detected as irrelevant and eliminated.

For the
Lasso and similar models (including Group Lasso),
\textit{exact}
screening techniques (i.e, techniques
  which don't mistakenly discard active predictive features) include those developed in
 \citep{elghaoui2010,lee2014exact,liu2014safe,wang2015lasso}. Inexact
screening techniques (e.g  \citep{tibshirani2010strong}) have also been
proposed in the literature.

Our proposed heuristic screening technique is inspired by the
\textit{Marginal screening} technique developed in Algorithm 1 of
 \citep{lee2014exact}, and operates as
follows. The data $(\X,\y)$ are standardized so that $\y$ has unit
variance and zero mean, likewise each row of the design matrix $\X$. To
ensure obtention of a smooth mask, a Gaussian-smoothed version
of $\X$ is used in the screening procedure (but not in the actual model
fit).
For each voxel $j$ (voxels are the features here) the
absolute dot-product $|\B{X}^T_j\y|$ of $\y$ with the $j$th column of
$\X$ is computed.
% This is used as the relevance measure.
For a given screening-percentile
$sp \in [0, 100]$ , the $sp$th percentile value of the
vector $|\B{X}^T\y| := (|\B{X}^T_1\y|, ..., |\B{X}^T_p\y|)$, denoted $p_{sp}(|\B{X}^T\y|)$,
is computed. The case $sp=100$ corresponds to full-brain analysis. 25
means we keep the quarter of the brain made of voxels with the highest
$|\B{X}^T_j\y|$ values, and so on.
% ; this is the threshold.
A brain-mask is then formed containing only those voxels $j$
for which $|\B{X}^T_j\y| \ge p_{sp}(|\B{X}^T\y|)$. Next, this brain-mask is
morphologically eroded
% (to remove isolated small patches)
and then
dilated, to obtain a more structured mask.  Figure
\ref{fig:screening} shows results of applying this screening heuristic
to various datasets, prior to model fitting.

\subsection{Early-stopping}
In each train sub-sample (for example a fold, in the case of $K$-fold
cross-validation) of the internal cross-validation loop for setting
the parameters of the GraphNet model \eqref{eq:opt_pb}, a pass is done
on the 2-dimensional parameter grid and each parameter pair
$(\alpha,\rho)$ is scored according to its prediction /
classification performance. For a fixed parameter pair
$(\alpha,\rho)$, an instance of problem \eqref{eq:opt_pb} is solved
iteratively using FISTA
\cite{beck09fista}. At each iteration, the prediction / classification
performance of the current (not yet optimal) solution $\hat{\B{w}}_k$ in
\eqref{eq:opt_pb} is computed. If in a time-window of 5 iterations
this score has not increased above an a priori fixed threshold, called
the \textit{early-stopping tolerance (es tol)}, then the optimization
process is halted for the currrent model parameter pair
$(\alpha,\rho)$ under inspection. This heuristic is motivated by the
intuition that, for a particular problem, sub-optimal solutions
$\hat{\B{w}}_k$ can give the same score as an optimal solution $\hat{\B{w}}$
(i.e ``statistical convergence'' happens before numerical
convergence). By default we set this early-stopping tolerance to
$-10^{-4}$ for classification and $-10^{-2}$ for regression
problems. A value of $+\infty$ (in fact, any value above 10, say)
corresponds to no early-stopping at all (i.e, solve problem
\eqref{eq:opt_pb} until numerical convergence).

\section{Experiments}
We experimented our early-stopping and (separately)
feature-screening heuristics on different MRI datasets.
All experiments were run using a single core of
  a laptop.

\paragraph{Regression.} The OASIS dataset
     \citep{marcus2007open} consists of a
    cross-sectional collection of 416 subjects aged 18 to 96. For each
    subject, 3 or 4 individual T1-weighted MRI scans obtained in
    single scan sessions are included.   A natural regression problem
    for this dataset is to predict the age of a subject from their
    anatomical data. To this end, we segmented the gray-matter from
    the anatomy of each subject (obtained from the T1 images), and
    used the gray-matter maps
    as features for predicting age. We split the 416 subjects into two
    equally-sized and age-balanced groups: a train set and a validation
    set. The GraphNet model  \citep{hebiri2011,grosenick2013} was fitted
    on the train set, with parameters
    ($\alpha$ and $\rho$ in \eqref{eq:opt_pb}) set internally via 8-fold
    cross-validation. The results for this experiment are shown in
    Figure \ref{fig:oasis}.

\paragraph{Classification.} The visual
    recognition dataset  \citep{haxby2001} is a popular block-design
    fMRI dataset from a
    study on face and object representation in human ventral temporal
    cortex.
It consists of 6 subjects with 12 runs per subject. In each run, the
subjects
passively viewed images of eight object categories, grouped
in 24-second blocks separated by intermittent rest periods. This
experiment is a classification task: predicting the object category
$\y$. We use a \textit{One-versus-Rest (OvR)} strategy. The design
matrix $\B{X}$ is made of
time-series from the full-brain mask of $p = 23\,707$ voxels over $n =
216$ TRs, of a single subject (subj1). We divided the 12 runs into 6
runs for training and 6 other runs for
validation. \textit{Leave-one-label-out} cross-validation was used for
selecting the model parameters $(\alpha, \rho)$. The results are
depicted in Figure \ref{fig:haxby}.

\section{Results}
We now summarize and comment the results of the experiments (refer to
section \ref{sec:experiments}).
Figure \ref{fig:oasis} shows the effects of early-stopping heuristic
and feature-screening heuristic on age prediction scores on the OASIS
dataset  \citep{marcus2007open} (416 subjects). We see that in the
internal cross-validation, stopping  the optimization procedure for
fixed $(\alpha, \rho)$ pair of regularization parameters, when test
score increases by $-10^{-2}$ or more is a good heuristic, and does just
as good as running the optimization until numerical convergence. 
 \begin{pagefigure}
   \includegraphics[width=1\linewidth]{figures/screening_weights.png}
  \caption{Predicting age from gray-matter concentration maps from the
    OASIS dataset  \citep{marcus2007open}. \textbf{Top}:
    Weights maps (solutions to problem \eqref{eq:opt_pb}).
% \textbf{N.B}: The long spiky undershoots in the prediction curves
% are indicative of outliers: subjects for which spatial preprocessing
% (tissue segmentation, normalization, etc.) failed.
\textbf{Bottom-left}: Mean Square Error (MSE) in age prediction, for
different subjects of the validation set, for  varying levels of the
early-stopping tolerance (``es tol'' for short), with the
screening-percentile (sp) held constant at 100
(full-brain). \textbf{Bottom-right}: MSE in age prediction, for
varying levels of the screening-percentile (sp).%  \textbf{Running
%   times}: Increasing \textit{est tol} (from $-10^{-4}$ to $10$): \textbf{100.2m, 171.4m, 188.8m, 289.6m}. For
% increasing $sp$ ($10$ to $100$): \textbf{44.2m, 81.3m, 186.5m, 341.3m}
}   
\label{fig:oasis}
\end{pagefigure}
Also (and independently), one gets similar prediction scores using as
little as a fifth of the brain volume ($sp=20$),
compared to using the full-brain ($sp=100$).
Figure \ref{fig:haxby} reports similar results for classification on
the visual recognition dataset  \citep{haxby2001}. Overall, we see from
Figures \ref{fig:haxby} and \ref{fig:oasis} that we can achieve upto
$10$-fold speedup with the proposed heuristics, with very little loss
in accuracy. Also, we see that contiguous groups of bars are roughly flat at the top, with a
    sligh increase from lower to high screening-percentile values. The
    case ``chair vs scramped'' is an exception, where a slightly reverse tendency
    if observed. A possible explanation is that $20$th percentile
    feature-screening already selects the right voxels (quasi-exact
    support recovery), and so including more voxels in the model can only hurt its
    performance...    

 \begin{pagefigure}
   \includegraphics[width=1\linewidth]{figures/screening_weights_haxby.png}
  \caption{Predicting age from gray-matter concentration maps from the
    OASIS dataset  \citep{marcus2007open}. \textbf{Top}:
    Weights maps (solutions to problem \eqref{eq:opt_pb}).
% \textbf{N.B}: The long spiky undershoots in the prediction curves
% are indicative of outliers: subjects for which spatial preprocessing
% (tissue segmentation, normalization, etc.) failed.
\textbf{Bottom-left}: Mean Square Error (MSE) in age prediction, for
different subjects of the validation set, for  varying levels of the
early-stopping tolerance (``es tol'' for short), with the
screening-percentile (sp) held constant at 100
(full-brain). \textbf{Bottom-right}: MSE in age prediction, for
varying levels of the screening-percentile (sp). \textbf{Running
  times}: Increasing \textit{est tol} (from $-10^{-4}$ to $10$): \textbf{100.2m, 171.4m, 188.8m, 289.6m}. For
increasing $sp$ ($10$ to $100$): \textbf{44.2m, 81.3m, 186.5m, 341.3m}}   
  \caption{Visual recognition dataset
     \citep{haxby2001}. \textbf{\textit{(a)}}: Weights maps
    % (maps of
    % regression coefficients $\hat{w}$)
    for the Face vs House contrast,
    for different early-stopping and univariate feature-screening
    thresholds. One can see that the supports of these maps for
    different values of the thresholds are quite similar to cases
    involving  no heuristic at all (the case where est $= 10$ and the
    where case sp $=100\%$).
    \textbf{\textit{(b)}, top-left}: Prediction scores as a function of
    the early-stopping tolerance (est), for different task contrasts.
    \textbf{\textit{(b)}, top-right}: Prediction scores as a function of
    the screening-percentile (sp), for different task contrasts.
    \textbf{\textit{(b)}, bottom-row}: Running times in minutes for the
    different thresholds of the heuristics.
    % As was to be expected, full-brain
    % (sp=100\%) is the most expensive scenario.
  }
  \label{fig:haxby}
\end{pagefigure}

\section{Conclusion}
we have presented heuristics that provide
speedups for optimizing GraphNet~\citep{hebiri2011,grosenick2013}  in the difficult
context of brain data. These heuristics are a win-win strat-
egy as they add speed without sacrificing the quality of
the predictions / classifications. In practice, we do a 20%
univariate feature-screening by default, which ensures a 5-
fold speedup over full-brain analysis, and independently of an
approximately 2-fold speedup obtained by the early-stopping
heuristic, leading to an overall 10-fold speedup. Our results
have been verified empirically on different MRI datasets. Our heuristics should
be applicable to other hard-to-optimize models like TV-L1~\citep{baldassarre2012,gramfort2013}.

The result of these numerous contributions on optimizing the SpaceNet model
\eqref{eq:opt_pb} have been implemented as part of the \textit{Nilearn} package
\citep{nilearn}.


% \newthought{We have seen} in Chapter~\ref{chap:stats_fmri} that%  encoding and decoding models take 
% as input  brain activation coefficients (also known as activation patterns or beta-maps). These are usually computed by means of the general linear model (GLM), which
% relies on a \mbox{data-independent} \emph{canonical} form of the hemodynamic response function
% (HRF).


% In this chapter we describe a novel method for the simultaneous estimation of HRF and activation coefficients based on low-rank modeling, forcing the estimated HRF to be equal across events or experimental conditions,
%  yet permitting it to differ across voxels. The estimation of this model leads to
% an optimization problem that we propose to solve with using a
% \mbox{quasi-Newton} method, exploiting fast gradient computations. 
% We compare 10 different HRF modeling methods in terms of encoding and decoding
% score on two different datasets. These results show that the \mbox{R1-GLM} model
% outperforms competing methods in both encoding and decoding
% settings, positioning it as an attractive method both from the points of view
% of accuracy and computational efficiency.

% \hspace{20pt}
% \begin{shaded}
% The contributions developed in this chapter have been published in:
% \begin{itemize}
% \item F. Pedregosa, M. Eickenberg, P. Ciuciu, and B. Thirion, \emph{``Data-driven HRF estimation for encoding and decoding models''} NeuroImage, Volume 104, 1 January 2015, Pages 209-220.

% \item F. Pedregosa, M. Eickenberg, B. Thirion, and A. Gramfort, \emph{“HRF estimation improves sensitivity of fMRI encoding and decoding models”} Proc. 3nd Int. Work. Pattern Recognit. NeuroImaging, 2013.
% \end{itemize}
% \end{shaded}

% \newpage
% \vspace*{\fill}
% \minitoc
% \vspace*{\fill}
% \newpage


% \section{Sparsity and smoothness priors for improved estimation in high dimensions}
% \newthought{Michel et al. 2011, Baldasarre et al. 2012, Gramfort et al. 2013, Abraham et al. 2013, Dohmatob et al. 2014(5), Varoquaux et al. 2016}, ...

% \begin{marginfigure}[4cm]
% \hspace{-20pt}\includegraphics[width=1.2\linewidth]{chapter_3/hrfs_age.pdf}
% \caption{
% 	The HRF can vary substantially between subjects, brain regions and age. In  \citept{colonnese2007development}, the authors studied the evolution of the HRF across age in rats. By comparing fMRI measurements with electrophysiological recordings, they observed two significant trends as age increased: growing amplitude and decreasing time to peak. In the figure, estimated HRF for three groups of rats (with age P13-15 < P20-30< Adult). Source:  \citepp{colonnese2007development}. A comparison of the HRF in human subjects was performed in~ \citepp{badillo2014multi}.
% }
% \end{marginfigure}


% fMRI acquisitions consist of successive brain scans, given in intervals ranging from 1 to 4 seconds. The extraction of time-independent \gls{activation coefficient} from the BOLD time course is commonly done with a model known as Linear General Model
% (GLM)~ \citepp{Friston1995}. While
% this approach has been successfully used in a wide range of studies, it does
% suffer from limitations~ \citepp{Poline2012}. For instance, the GLM commonly
% relies on a \mbox{data-independent} \emph{reference} form of the hemodynamic response function
% (HRF) to estimate the activation coefficient (also known as \emph{canonical HRF}). However it is
% known~ \citepp{Handwerker2004,Badillo2013} that the shape of this response function
% can vary substantially across subjects, age and brain regions. This suggests that an adaptive modeling of this
%  response function should improve the accuracy of subsequent analysis.

% % \emph{feature-extraction} model that extracts 

% % In this section we describe a method that allows to estimate time-independent \gls{activation coefficient} given the BOLD time course. {\blue Feature extraction}. This model is known as the \emph{general linear model}~ \citepp{Friston1995}. In this chapter we describe the main assumptions behind this model: a known form of the hemodynamic response function and the linear-time-invariant property between the BOLD signal and the neural response. 

% % We have seen in Chapter 2 that both encoding and decoding models take as input voxel-wise activation coefficients. These are commonly are computed by means of the General Linear Model
% % (GLM)~ \citepp{Friston1995}. While
% % this approach has been successfully used in a wide range of studies, it does
% % suffer from limitations~ \citepp{Poline2012}. For instance, the GLM commonly
% % relies on a \mbox{data-independent} \emph{reference} form of the hemodynamic response function
% % (HRF) to estimate the activation coefficient (also known as \emph{canonical HRF}). However it is
% % known~ \citepp{Handwerker2004,Badillo2013} that the shape of this response function
% % can vary substantially across subjects, age and brain regions. This suggests that an adaptive modeling of this
% %  response function should improve the accuracy of subsequent analysis.

% To overcome the aforementioned limitation, Finite Impulse Response (FIR) models have been
% proposed within the GLM framework~ \citepp{Dale1999,Glover1999}.
% These models do not assume any particular shape for the HRF and amount to
% estimating a large number of parameters in order to identify it. 
% While the FIR-based modeling makes it possible to estimate the
% activation coefficient and the HRF simultaneously, the increased flexibility
% has a cost. The estimator is less robust and prone to overfitting, i.e. to generalize poorly to unseen data. 
% In general, FIR
% models are most appropriate for studies focused on the characterization of the
% shape of the hemodynamic response, and not for studies that are primarily
% focused on detecting activation~ \citep[Chapter~5]{Poldrack}.

% Several strategies aiming at reducing the number of degrees of freedom of the
% FIR model - and thus at limiting the risk of overfitting - have been proposed.
% One possibility is to constrain the shape of the HRF to be a linear
% combination of a small number of basis functions. A common choice of basis is 
% formed by three elements consisting of a reference HRF as well as its time and dispersion
% derivatives~ \citepp{friston1998nonlinear}, although it is also possible to compute a
% basis set that spans a desired function
% space~ \citepp{Woolrich2004}. More generally, one can also define a parametric
% model of the HRF and estimate the parameters that best fit this
% function~ \citepp{Lindquist2007}. However, in this case the estimated HRF may no longer be a linear function of the input parameters. 

% Sensitivity to noise and overfitting can also be reduced through
% regularization. For example, temporal regularization has been used in the
% smooth FIR ~ \citepp{Goutte2000,Ciuciu2003,Casanova2008} to favor solutions with
% small second order time derivative. These approaches require the setting of
% one or several hyperparameters, at the voxel or potentially at the parcel
% level (if several voxels in a pre-defined parcel are assumed to share some aspects of the HRF time course). Even if efficient techniques such as generalized   
% \mbox{cross-validation}~ \citepp{golub1979generalized} can be used to choose the
% regularization parameters, these methods are inherently more costly than 
% \mbox{basis-constrained} methods. \mbox{Basis-constrained} methods also require
% setting the number of basis elements; however, this parameter is not
% continuous (as in the case of regularized methods), and in practice only few
% values are explored: for example the 3-element basis set formed by a reference HRF
% plus derivatives and the FIR model.  This paper focuses on basis-constrained
% regularization of the HRF to avoid dealing with hyperparameter selection with
% the goal of remaining computationally attractive.  A different approach to
% increase robustness of the estimates consists in linking the estimated HRFs
% across a predefined brain parcel, taking advantage of the spatially dependent nature of
% fMRI~ \citepp{Wang2013}. However, \mbox{hemodynamically-informed}
% parcellations~ \citepp{Chaari2012,Badillo2013a} rely on the computation of 
% a large number of estimations at the voxel or \mbox{sub-parcel} level.
% In this setting, the development of voxel-wise estimation procedures is complementary to the
% development of parcellation methods in that more robust estimation
% methods at the voxel level would naturally translate into more 
% robust parcellation methods. In this thesis we focus on voxel-wise
% estimation methods.


% \paragraph{Contribution}

% In this chapter we have described a method for the simultaneous estimation of HRF and activation coefficients based on low-rank modeling. While the assumptions of this model are not novel (cf.~ \citepp{Makni2008,vincent2010spatially,Degras2014}), the formulation of this model as a least squares problem with a rank-one constraint is a novel contribution. This formulation allows to efficiently solve the problem using gradient-based methods.
% Finally, we evaluate the proposed model on three publicly available datasets. 

% % {\blue With respect to the work published in ~ \citepp{Pedregosa2015209}, we have included in this chapter the results on a new datasets and examined the gain obtained by this model across different regions of the brain}.

% \clearpage
\bibliographystyle{plainnat}
\bibliography{bib_all}
